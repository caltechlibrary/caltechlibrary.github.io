{
  "page": 1,
  "total_pages": 1,
  "has_more": false,
  "next_page": null,
  "values": [
    {
      "content": "\n# OpenSearch Description Documents: What, Why and How\n\nBy R. S. Doiel, 2025-08-11\n\nWith the steady declining most commercial search engines over the last decade my interest in site specific search has been grown. When the web was young search meant site search.  Overtime search as a service arrived where it is considered an integral part of the web. The trouble now is that commercial search isn't about finding the thing you are looking forward it is about selling attention and surveillance. While systems like Google Scholar still exist the question that nags at me is for how long? What are out options today?\n\nSite search didn't disappear. It lives in content management systems like Drupal and WordPress. It lives in library systems like ArchivesSpace and Invenio RDM. It isn't the new hotness but it works pretty good. As libraries and archives continue the trend of taking advantage of static web site implementations site search can live there too.  This begs the question, how can we make it more convenient than maintaining a bookmark list and always going directly to the site to retrieve results?\n\nWe're in luck! An old solution to exposing site search in the web browser remains supported today. [OpenSearch Description Documents](https://en.wikipedia.org/wiki/OpenSearch_(specification) \"this is about the OpenSearch Description document to not the Amazon sponsored Open Source project\") is a specification that dates back to 2005. It is not a coincidence that the Omnibox arrives by 2008. The humble URL box is the thing we use to navigate the web. It's the on ramp. It's also where search starts for most people. The specification for OpenSearch Description Documents is a specification to make that possible.\n\nIt is important not to confuse **OpenSearch Description Document** with OpenSearch the fork by Amazon of ElasticSearch. It is a simple XML document that can be used by Firefox, Safari and Chrome to include your site search as a search engine (NOTE: Safari and Chrome browser make you jump through a few hoops before it sites along side Google, DuckDuckGo and the others).  The really nice thing about take the effort with Safari and Chrome is that you don't need to navigate to the website to take advantage of site search. Anytime you are in the URL box (Omnibox) it's right there ready to be used.\n\nThere are three parts to the dance of making your URL Box (Omnibox) gain first class status along side commercial search.\n\n1. You need an OpenSearch Description Document (XML file) on your site\n2. You need to include a link to it in the head element of your search web page (or other HTML pages)\n3. You need to have a search engine to link to that can response to a URL and return results\n\n## What does the OpenSearch Description Document Look Like?\n\nThe OpenSearch Description Document is a short XML document with a few required fields. Here's a short version.\n\n~~~XML\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<OpenSearchDescription xmlns=\"http://a9.com/-/spec/opensearch/1.1/\">\n  <ShortName>DLDLabs</ShortName>\n  <Description>Search Caltech Library DLD Labs</Description>\n  <Url type=\"text/html\" method=\"get\" template=\"https://caltechlibrary.github.io/search.html?q={searchTerms}\"/>\n  <InputEncoding>UTF-8</InputEncoding>\n  <Image height=\"16\" width=\"16\">https://caltechlibrary.github.io/favicon.ico</Image>\n</OpenSearchDescription>\n~~~\n\nThe outer XML element is the **OpenSearchDescription**. Inside it you include a **ShortName**, **Description**, **Url**,\n**InputEncoding** and **Image** elements. The last two are optional but I think are helpful.  In the DLD Labs website I've called this document \"osd.xml\" but you can name it whatever you like. The link element you include in the head element of HTML pages is what lets the web browser know it is available.\n\n**OpenSearchDescription**\n: The outer XML element holding the details about your search service.\n\n**ShortName**\n: This is a short, less than 14 alpha numeric characters used to refer to your search service.\n\n**Description**\n: This is an up to 1024 character description of your search service.\n\n**Url**\n: This describes the URL template needed to form a search URL. The `{searchTerms}` is replaced with the query terms taken from the URL box or Omnibox\n\n**InputEncoding**\n: This sets the character encoding your search engine supports, UTF-8 is the likely choice you want.\n\n**Image**\n: This is the image associated with your search. Browser may display it when you URL Box/Omnibox is set to search your site.\n\n## How do I link your OpenSearch Description Document to your web page?\n\nIf you have an **OpenSearch Description Document** available you need to point to it from a web page. All you need is a simple link element in the head of the HTML page. Here's the link used on the DLD Labs site.\n\n~~~html\n<link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"https://caltechlibrary.github.io/osd.xml\" title=\"Search DLD Labs\">\n~~~\n\nIn the link we use the \"rel\", \"type\", \"href\" and \"title\" attributes to connect the page with the OpenSearch Description Document (abbr. OSD). Once the OSD document is linked it can be discovered by the web browser. In this case it tells the browser the OSD document can be found at \"https://caltechlibrary.github.io/osd.xml\".\n\n### Trouble shooting OSD\n\nAn important think to remember of the OpenSearch Description Document is that it is XML. XML, unlike HTML, isn't forgiving. If you have a mismatched quote or a type in an element name it's not going to work. \n\nThe Mozilla Developer Network maintains excellent documentation on the specification at <https://developer.mozilla.org/en-US/docs/Web/XML/Guides/OpenSearch>.\n\n## Adding your site search to your web browser\n\n### Firefox\n\nFirefox works seamlessly with OSD documents.  You land on a web page where it is linked in. If you clear the URL Box and pull down the list of available search engines, newly discovered on appear at the top.\n\n![Picture of the pull down for search choices](FireFox_URL_Box.png)\n\nThe image on the left with the downward caret is the menu that exposes the search options. Click on it and you'll see the list. If you see an icon with a plus symbol overlaid in the upper right of the icon it's a new search you can add to the ones available. Clicking on it will add it as an option to your web browser.\n\nYou can also add a search engine by going to <about:preferences#search>, scroll down to Search Shortcuts and press the \"add\" button. This will let you add the search manually.\n\n### Safari\n\nSafari has locked down this feature but still supports it. Apple just makes it hard to find and change it.\n\n1. Open the URL of the page that links to the OpenSearch Description Document\n2. Go to the Safari menu, click on settings\n3. Click on Search\n4. Make sure \"Enable Quick Website Search\" is checked\n\nIf this is enabled you can now type in the host-name of the website, a space followed by your search terms. Bingo you're now search the website.\n\n### Chrome\n\nChrome requires the most steps to enable your site search. This is not surprising given the parent company. \n\n1. Open the URL of the page that links to the OpenSearch Description Document\n2. Click on Chrome's menu and open the settings\n3. Click on Search Engine\n4. Just below the box where you set your default search engine is a box labeled \"Manage search engines and site search\", click on it.\n5. Scroll way down on the page and you should see the new site search in the \"Inactive Shortcuts\" list. Click the button labeled \"activate\" for your site search.\n\nThe last step will move the site search list. There will be a pencil icon on the right side. You can give it a shortcut name by clicking on the pencil, e.g. \"@dldlbs\" is what I use for searching DLD Labs site from Chrome's Omnibox.\n\n\n## Integrating site search for static websites\n\nIf you have Solr, ElasticSearch, Drupal, WordPress, ArchivesSpace, Invenio RDM or EPrints search is running on your server someplace. What about your static website?  Do you have to run a separate search service?  No unless your site has multiple hundreds of thousands of pages to search.\n\nWith the rise in popularity of static websites since 2010s browser side search has become an option. Around 2011 [Oliver Nightingale](https://github.com/olivernn) pioneered browser side search with [LunrJS](https://lunrjs.com). More recently [CloudCannon](https://cloudcannon.com/), created an Open Source tool called [PageFind](https://pagefind.app). This takes the LunrJS approach and adds rockets. LunrJS could support up to about 10,000 pages before it really bogged down. PageFind goes way beyond that. PageFind can even integrated search indexes across multiple website! Here's an example of the MDN website's search implemented with PageFind, <https://mdn.pagefind.app/>.\n\nPageFind has good [documents](https://pagefind.app/docs) for setting up a simple search page. It works well. Open the URL of the search page, you are presented with a search box, type in a search and you get results.  The problem is the page's URL doesn't let you go straight to search results.  There's an easy fix for that.   When the page loads the same JavaScript you use to enable the search page to work can also check the URL's query parameters for a query field and search term. Example <https://caltechlibrary.github.io/search.html?q=dataset%20web%20components>. This has two benefits\n\n1. You can use it as a pattern as a template in the OpenSearch Description Document in the **Url** element. \n2. You can bookmark search results\n\n### Setting up PageFind to support URL search queries\n\nBasically want to implement two things beyond the default PageFind example.  We need to retrieve the query parameter. I'm using the field \"q\" because that is a common idiom for most site search systems.  If that field is present and populated then we need to \"triggerSearch\" in the PageFindUI object.  The second thing is when we're typing in the web form the URL should update just before the search results are fetched. That is done by adding a \"processTerm\" hook when we configure the PageFundUI object.\n\nHere's a simple version of the JavaScript we run on DLD Lab's search.html page.\n\n~~~JavaScript\n// Import PageFindUI\n\n// Function to extract query parameters from the URL\nfunction getQueryParam(name) {\n  const urlParams = new URLSearchParams(window.location.search);\n  return urlParams.get(name);\n}\n\n// Extract the query parameter\nconst searchQuery = getQueryParam('q');\n\n// Function to update the URL with the search query\nfunction updateURLWithQuery(query) {\n  const newUrl = new URL(window.location.href);\n  newUrl.searchParams.set('q', query);\n  window.history.pushState({ path: newUrl.href }, '', newUrl.href);\n}\n\n// When the page is fully loaded setup the PageFindUI object\nwindow.addEventListener('DOMContentLoaded', (event) => {\n    const searchUI = new PagefindUI({\n        element: \"#search\",\n        highlightParam: \"highlight\",\n        mergeIndex: [\n            {\n                bundlePath: \"https://caltechlibrary.github.io/pagefind\",\n                baseUrl: \"/\"\n            },\n        ],\n        processTerm: (term) => {\n            updateURLWithQuery(term);\n            return term;\n        }\n    });\n\n    if (searchQuery) {\n        searchUI.triggerSearch(searchQuery);\n    }\n});\n~~~\n\nIf you wanted to integrate you're own PageFind search that supports query URL you could use this code but replace the \"bundlePath\" with the URL to your website's PageFind indexes. You can see the expanded version of our implementation at <https://caltechlibrary.github.io/modules/search.js>. In our implementation we also support searching the documentation sites for some of our projects.\n\n## More arguments for browser side search\n\nWhen the search engine runs on the server you're at the mercy of the people who control the server. Searches are a treasure trove for the surveillance economy. With PageFind the search runs in your browser. It only leaves your browser to select the next partial index needed.  This significantly reduces the exposed data. You might be able to infer a set of search terms but you're not going to see the specific search string typed in.\n \n### What is Page Find doing differently?\n\nPageFind, the command line program, is run after you stage your website but before you publish it. It crawls the HTML pages on disk and extracts the content you identified to be in included in your search indexes. PageFind, unlike LunrJS, does not encode the index as JSON. Instead is includes the indexes as a sequence of WASM (Web Assembly) files. This allows for dense indexes. Another step that sets PageFind apart is the indexes generated are partitioned. You don't download all indexes to the browser only the ones you need to based on the search terms.  When you stop typing for a moment PageFind JavaScript running in your web browser figures out which indexes are useful next and retrieves.  Clever stuff.\n\nWhen we add support for using a query URL PageFind only sends the query on page load. This does mean you're going to wait for the indexes to be retrieved based on the completed query string. That delay is noticeable but I think a reasonable trade off.\n\n## Why is the OpenSearch description document important?\n\nAs researchers, archivist, librarians and users of the research materials skipping the ad-tech of Google et el means smoother workflows when looking things up on line. It also means websites that are focused on our specific research interests can be used as a primary search resource if they include the description of their site search.  While being able to type in the URL Box might seem like   a convenience it's the small user interface improvement that when understood can smooth out flow of retrieving research resources online. This is true regardless of with or not the site is a \"dynamic\" website (e.g. WordPress, RDM, ArchivesSpace) or a static one (e.g. \"cell atlas\").\n",
      "data": {
        "abstract": "This post explains the OpenSearch Description Document and how to integrate it for a website using PageFind. It touches on how to use site search defined by the OpenSearch Description Document in Firefox, Safari and Chrome. \n",
        "author": "rsdoiel@caltech.edu (R. S. Doiel)",
        "copyrightHolder": "California Institute of Technology",
        "copyrightYear": 2025,
        "dateCreated": "2025-08-08T00:00:00.000Z",
        "dateModified": "2025-08-11",
        "datePublished": "2025-08-11",
        "keywords": [
          "site search",
          "opensearch description document",
          "pagefind",
          "web browser"
        ],
        "title": "OpenSearch Description Documents"
      },
      "url": "posts/2025/08/11/Opensearch_Description_Document.json"
    },
    {
      "content": "\n# Setting up RDM 13 on macOS\n\nUPDATE: Added notes about install NodeJS via nvm, 2025-08-05 RSD\n\nThis is a summary of my experience bringing up a vanilla Developer Invenio RDM 13 experience on macOS 15.6 running on a M1 Mac Mini. I use Mac Ports for supporting development.  I use uv to manage Python.\n\nThe instructions I was following are found at <https://inveniordm.docs.cern.ch/install/>. I am doing a \"developer\" setup.\n\nIf you run into problems I highly recommend making sure you have a \"clean\" system before proceeding. This is especially true if you've had more than one version of Python on your machine (regardless of how), installed RDM in the past or have had older versions of Docker Desktop before. RDM is complicated and brittle even run in containers. Having a clean system is essential to a positive RDM experience.\n\n## Make sure you have a clean system\n\n- removed Docker Desktop and all related files\n- removed all Mac Ports pythons\n- removed all pythons via uv\n- removed uv\n- remove ImageMagick and it's dependencies via Mac Ports, make sure your PATH and LD_LIBRARY_PATH are clean\n- make sure any stale environment (e.g. .bashrc, .profile, etc) are removed\n- rebooted system\n- Confirm there is no Docker, non-system Python, ImageMagick\n- Confirm your environment is clean (do you have references to removed things in your PATH or LD_LIBRARY_PATH?)\n\n## Preparing your machine\n\nOnce I had a clean system here's the steps I took.\n\n1. Install NodeJS via `nvm`, see <https://nodejs.org/en/download>, select macOS and using \"nvm\" with \"npm\"\n2. Install of [uv](https://docs.astral.sh/uv/getting-started/installation/)\n3. Install python3.12 as default using uv, `uv python install 3.12 --default`\n4. Install Docker Desktop\n5. Install ImageMagick7 via Mac Ports, `sudo port install ImageMagick7`\n6. Install cairo via Mac Ports, `sudo port install cairo`\n7. Make sure that Mac Ports `libcairo.2.dylib` is symbolically linked to `/usr/local/lib`\n8. Install openssl via Mac Ports, `sudo port install openssl`\n\n### NodeJS for a system written in Python?\n\nCurrently the Invenio Project uses NodeJS for managing React components. You'll want to have NodeJS available. I recommend installing NodeJS via \"nvm\" (Node version manager). It'll save you some grief and let you use NodeJS for other projects which might need more recent versions.\n\nHere's what I do in a macOS Terminal window.\n\n~~~shell\nopen https://nodejs.org/en/download\n~~~\n\nThen in the web browser I make sure the download page is referencing macOS, nvm and npm. I think this is the default but I've visited the site so many times over the years it could be I have a cookie set to show that choice. \n\nThe window will show a shell script/session you can run. At the time of writing this it look like this for me.\n\n~~~shell\n# Download and install nvm:\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash\n\n# in lieu of restarting the shell\n\\. \"$HOME/.nvm/nvm.sh\"\n\n# Download and install Node.js:\nnvm install 22\n\n# Verify the Node.js version:\nnode -v # Should print \"v22.18.0\".\nnvm current # Should print \"v22.18.0\".\n\n# Verify npm version:\nnpm -v # Should print \"10.9.3\".\n~~~\n\n## Installing RDM 13\n\nYou should now be prepared to follow the instructions at <https://inveniordm.docs.cern.ch/install/#quick-start>.\n\nWhen you follow the Quick Install pick the `uv` example under **Install the CLI tool** heading.\n\n- Make a directory to hold your install\n- Run the setup sequence from Quick Start\n\nWhat follows are the commands (with comments) that I ran to bring RDM 13 up on my machine after I cleaned it up and prepared\n\n~~~\nmkdir rdm_test                  # this is my test directory\ncd rdm_test\nuv tool install invenio-cli\ninvenio-cli check-requirements  # If this doesn't pass you probably have a dirty system still\ninvenio-cli init rdm            # Accept all the defaults for testing\ncd my-site\ninvenio-cli check-requirements --development  # If this doesn't pass ...\n# Install Python and JavaScript packages, you'll see warns about depreciated packages and Node stuff (that's normal)\ninvenio-cli install\n# Set up containerized database, cache, OpenSearch, etc. You'll see warnings about depreciated stuff\ninvenio-cli services setup\n# Serve the application locally through a development server. This will result in a bunch of logged output\n# and continue for a while. Be patient. There will be warning sprinkled in there too.\ninvenio-cli run\n~~~\n\nIn another terminal windows point your browser at <https://127.0.0.1:5000/>, click through the warnings about the self signed cert.\n\n~~~\nopen https://127.0.0.1:5000/\n~~~\n\nYou should now have a running vanilla RDM 13 instance up. To bring it down do that following.\n\n~~~\n# To stop the application server:\n# in terminal running invenio-cli run\n^C [CTRL+C]\n# ---\n# To stop the service containers:\ninvenio-cli services stop\n# ---\n# To destroy the service containers\n# (this will lose ALL data):\ninvenio-cli services destroy\n~~~\n",
      "data": {
        "abstract": "This is a summary of the steps I took to successfully setup RDM 13 on my M1 Mac Mini running macOS 15.6. I use Mac Ports for dependencies.\n",
        "author": "R. S. Doiel",
        "dateCreated": "2025-08-01",
        "dateModified": "2025-08-05",
        "datePublished": "2025-08-01",
        "keywords": [
          "Invenio",
          "RDM"
        ],
        "title": "Setting up RDM 13 on macOS M1 Mac Mini"
      },
      "url": "posts/2025/08/01/Setting_up_RDM_13_on_macOS.json"
    },
    {
      "content": "\n# Language models and \"fielded\" search\n\nBy R. S. Doiel, 2025-04-10\n\nThis an interesting use of an LLM, it means that the old \"advanced search\" UI or fielded search UI can be made to feel like a single box search, <https://simonwillison.net/2025/Apr/9/an-llm-query-understanding-service/ >. From Simon Willison's Weblog,\n\n> \"Many times, even a small open source LLM will be able to turn a search query into reasonable structure at relatively low cost.\"\n\nSo does this mean the huge Solar/OpenSearch indexes aren't needed?  Probably not but it does mean that we can build much more effective search and retrieval systems before requiring a full text search engine. Running an Ollama instance with an appropriate frugal model is almost trivial.\n",
      "data": {
        "author": "rsdoiel@caltech.edu (R. S. Doiel)",
        "datePublished": "2025-04-10",
        "keywords": [
          "LLM",
          "search"
        ],
        "title": "Language models and \"fielded\" search"
      },
      "url": "posts/2025/04/11/language_models_and_fielded_search.json"
    },
    {
      "content": "\n# Google Chrome and DNS problems\n\nChrome can be problematic. It is not as reliable as it used to be.  Recently we've run into issues where you can't reach Caltech Library web resources even through they are up and available via other browsers like Safari. There are a couple things you can try before resorting to a clean reinstall of Google Chrome.\n\n1. Clear you browser caches, this is accomplished from the \"settings\" page. Google seems to change its contents an layout regularly. You'll have to open the \"settings\" and visual inspect the elements to see how to empty your caches.\n2. Chrome's less obvious chrome://net-internals settings page. This page contains additional caches you can try to clear\n3. If that doesn't work remove Chrome completely then try a fresh install (the solution that ultimately worked)\n\n## Removing Google Chrome on macOS completely\n\nBasic idea.\n\n1. Quit Chrome if running, you can check the \"Force Quit\" list to make sure it's not running\n2. Locate the Chrome related directories and remove them.\n\nBelow is a description of using your mouse and finder to do the work. The first part is easy the latter part more challenging as macOS really doens't like making this easy (shame on them).\n\n### Finder approach\n\nStart up the Terminal App. Changing into the \"Library\" directory (aka folder) open the folder in finder using the open command, open .. This will let you see what is inside the Library folder (often hidden in the regular finder view of folders).\n\n~~~shell\ncd Library\nopen .\n~~~\n\nRemove the main application. You can use finder to find \"Google Chrome\" and drag it to the trash.\n\n- \"/Applications/Google Chrome.app\"\n\nYou need to find and remove the following \"Google\" folders. Normally the \"Library\" folder isn't listed in your finder window. What I do is start up \"Terminal\" then change directory into the library folder and use the \"open .\" command.\ncd Library\nopen .\n\nNow we should be able to find these three folders and drag them to the trash.\n\n- \"$HOME/Library/Google\"\n- \"$HOME/Library/Application Support/Google\"\n- \"$HOME/Library/Caches/Google/\"\n\nThe next back of folders are trickier. They may more may not exist. The first two can be found using the finder and dragged to the trash. The lasts two you have to search for. The \"*\" is a wild card. I find it easier to locate them\nusing the terminal and the old fashioned Unix \"find\" command.\n\n- \"$HOME/Library/Caches/chrome_crashpad_handler\"\n- \"$HOME/Library/HTTPStorages/chrome_crashpad_handler\"\n- \"$HOME/Library/Application Support/Code/CachedData/*/chrome\"\n- \"$HOME/Library/HTTPStorages/com.google.*\"\n\nOnce these folders are all in the trash you can empty the trash (make take a little while). \n\n### Using the shell commands\n\nWhile this is more typing (which you can minimize by cutting and pasting) I found it less frustrating. These commands are all executed from your Terminal window. You can cut and paste the lines one by one or you can save them in a plain text file, e.g. \"remove_chrome.bash\" and run them with the command \"sh remove_chrome.bash\".\n\nIf you run the script is can a while (30 to 120 seconds).\n\n~~~shell\n#!/bin/bash\n\nsudo rm -r /Applications/Google\\\\ Chrome.app/\nsudo rm -fR \"$HOME/Library/Google\"\nsudo rm -fR \"$HOME/Library/Application Support/Google\"\nsudo rm -fR \"$HOME/Library/Caches/Google/\"\n\nsudo rm -fR \"$HOME/Library/Caches/chrome_crashpad_handler\"\nsudo rm -fR \"$HOME/Library/HTTPStorages/chrome_crashpad_handler\"\nsudo rm -fR \"$HOME/Library/Application Support/Code/CachedData/*/chrome\"\nsudo rm -fR \"$HOME/Library/HTTPStorages/com.google.*\"\n~~~\n\nYou can check to make sure everything is gone with the script.\n\n~~~shell\nsudo ls /Applications/Google\\\\ Chrome.app/\nsudo ls \"$HOME/Library/Google\"\nsudo ls \"$HOME/Library/Application Support/Google\"\nsudo ls \"$HOME/Library/Caches/Google/\"\n\nsudo ls \"$HOME/Library/Caches/chrome_crashpad_handler\"\nsudo ls \"$HOME/Library/HTTPStorages/chrome_crashpad_handler\" \nsudo ls \"$HOME/Library/Application Support/Code/CachedData/*/chrome\"\nsudo ls \"$HOME/Library/HTTPStorages/com.google.*\"\n~~~\n",
      "data": {
        "abstract": "Quick notes on a \"DNS\" problem specific to Google Chrome\nand it got solved.\n",
        "author": "rsdoiel@caltech.edu (R. S. Doiel)",
        "dateCreated": "2025-04-01",
        "dateModified": "2025-08-04",
        "datePublished": "2025-04-01",
        "keywords": [
          "DNS",
          "Chrome"
        ],
        "title": "Google Chrome and DNS problems"
      },
      "url": "posts/2025/04/01/Google_Chrome_DNS_problems.json"
    },
    {
      "content": "\n# Building Web Components using Large Language Models\n\nby R. S. Doiel, 2025-03-13\n\nI started playing around with Mitral Chat to create web components for some library applications.  It’s a non-trivial process, not sure it is faster than me just write them from scratch but I am so far happy with the results which you can see <https://github.com/caltechlibrary/CL-web-components>.  There are two, one is called “a_to_z_ul.js” which lets you wrap a simple UL list and have it be an “A to Z” list. The second is call “csvtextarea.js” which is a component that wraps a TEXTAREA input element that contains CSV data and renders a table where you can edit the cells.  The wrapping of existing components was not the LLM’s suggestion, that was my idea but the generated JavaScript is from the LLM based on my prompts.\n\nOne of the challenges I ran into was the lack of reproducibility even for the same prompts. The second was I had to use a paid subscription to Mistral as the free one available from DuckDuckGo wasn’t enough. I compared a few other LLM as I learned to use the chat interface and Mistral price for results was the best for my tests.\n\nAfter getting a feel for using Mistral I tried running the original prompts against Mistral using Ollama. The results are different. It is cleared the “paid” platforms of open sourced models have significant enhancements in the models.\n\nI worry about the energy consumed in running the models let along refining them.\n\nWhile I like the results I got for this specific test I am on the fence about the general usefulness for libraries, archives and museums.\n",
      "data": {
        "abstract": "A brief overview of an experiment resulting in <https://github.com/caltechlibrary/CL-web-components>.\nTwo web components were developed that use a progressive enhancement approach, CSVTextarea and A_to_ZUL.\n",
        "author": "rsdoiel@caltech.edu (R. S. Doiel)",
        "dateCreated": "2025-03-13",
        "dateModified": "2025-08-04",
        "datePublished": "2025-03-13",
        "keywords": [
          "web components",
          "browsers",
          "HTML",
          "CSS",
          "JavaScript",
          "LLM"
        ],
        "title": "Building Web Components using Large Language Models"
      },
      "url": "posts/2025/03/13/Building_Web_Components_using_LLM.json"
    }
  ]
}